from functools import partial
import tensorflow as tf


def mean_squared_loss(network, params, x):
    """Computes the mean squared loss between true and predicted parameters.

    network  : tf.keras.Model
        A neural network with a single output vector (posterior means)
    params   : tf.Tensor of shape (batch_size, n_out_dim)
        Data-generating parameters, as sampled from prior
    x        : tf.Tensor of shape (batch_size, N, x_dim)
        Synthetic data sets generated by the parameters

    Returns
    -------
    loss : tf.Tensor
        A single scalar value representing the mean squared loss, shape (,)
    """

    params_pred = network(x)
    loss = tf.reduce_mean(tf.reduce_sum((params - params_pred) ** 2, axis=1))
    return loss

def meta_amortized_loss(network, model_indices, params, sim_data):
    """Computes the loss of a MetaAmortizer instance (KL + LogLoss)

    Parameters
    ----------
    network  : tf.keras.Model
        A neural network with a single output vector (posterior means)
    params   : tf.Tensor of shape (batch_size, n_out_dim)
        Data-generating parameters, as sampled from prior
    x        : tf.Tensor of shape (batch_size, N, x_dim)
        Synthetic data sets generated by the parameters

    Returns
    -------
    loss : tf.Tensor
        A single scalar value representing the meta amortized loss, shape (,)
    """

    out_inference, out_evidence = network(model_indices, params, sim_data)
    if out_inference is not None:
        z, log_det_J = out_inference
        kl_loss = tf.reduce_mean(0.5 * tf.square(tf.norm(z, axis=-1)) - log_det_J)
    else:
        kl_loss = 0
    
    if out_evidence is not None:
        model_probs = out_evidence
        model_probs = tf.clip_by_value(model_probs, 1e-15, 1 - 1e-15)
        log_loss = -tf.reduce_mean(tf.reduce_sum(model_indices * tf.math.log(model_probs), axis=1))
    else:
        log_loss = 0
    return kl_loss + log_loss


def heteroscedastic_loss(network, params, x):
    """Computes the heteroscedastic loss between true and predicted parameters.

    Parameters
    ----------
    network  : tf.keras.Model
        A neural network with a single output vector (posterior means)
    params   : tf.Tensor of shape (batch_size, n_out_dim)
        Data-generating parameters, as sampled from prior
    x        : tf.Tensor of shape (batch_size, N, x_dim)
        Synthetic data sets generated by the parameters

    Returns
    -------
    loss : tf.Tensor
        A single scalar value representing the heteroscedastic loss, shape (,)
    """

    pred_mean, pred_var = network(x)
    logvar = tf.reduce_sum(0.5 * tf.math.log(pred_var), axis=-1)
    squared_error = tf.reduce_sum(0.5 * tf.math.square(params - pred_mean) / pred_var, axis=-1)
    loss = tf.reduce_mean(squared_error + logvar)
    return loss


def kl_latent_space(network, *args):
    """Computes the Kullback-Leibler divergence (Maximum Likelihood Loss) between true and approximate
    posterior using simulated data and parameters.

    Parameters
    ----------
    network   : tf.keras.Model
        A single model amortizer
    *args
        List of arguments as inputs to network (e.g. model_indices, params, sim_data)

    Returns
    -------
    loss : tf.Tensor
        A single scalar value representing the KL loss, shape (,)

    Examples
    --------
    Parameter estimation

    >>> kl_latent_space(net, params, sim_data)

    Model comparison

    >>> kl_latent_space(net, model_indices, sim_data)

    Meta

    >>> kl_latent_space(net, model_indices, params, sim_data)
    """

    z, log_det_J = network(*args)
    loss = tf.reduce_mean(0.5 * tf.square(tf.norm(z, axis=-1)) - log_det_J)
    return loss


def log_loss(network, model_indices, sim_data, lambd=1.0):
    """Computes the logloss given output probs and true model indices m_true.

    Parameters
    ----------
    network       : tf.keras.Model
        An evidential network (with real outputs in ``[1, +inf]``)
    model_indices : tf.Tensor of shape (batch_size, n_models)
        True model indices
    sim_data      : tf.Tensor of shape (batch_size, n_obs, data_dim) or (batch_size, summary_dim) 
        Synthetic data sets generated by the params or summary statistics thereof
    lambd         : float in [0, 1]
        The weight of the KL regularization term

    Returns
    -------
    loss : tf.Tensor
        A single scalar Monte-Carlo approximation of the regularized Bayes risk, shape (,)
    """

    # Compute evidences
    alpha = network(sim_data)

    # Obtain probs
    model_probs = alpha / tf.reduce_sum(alpha, axis=1, keepdims=True)

    # Numerical stability
    model_probs = tf.clip_by_value(model_probs, 1e-15, 1 - 1e-15)

    # Actual loss + regularization (if given)
    loss = -tf.reduce_mean(tf.reduce_sum(model_indices * tf.math.log(model_probs), axis=1))
    if lambd > 0:
        kl = kl_dirichlet(model_indices, alpha)
        loss = loss + lambd * kl
    return loss


def kl_dirichlet(model_indices, alpha):
    """Computes the KL divergence between a Dirichlet distribution with parameter vector alpha and a uniform Dirichlet.

    Parameters
    ----------
    model_indices : tf.Tensor of shape (batch_size, n_models)
        one-hot-encoded true model indices
    alpha         : tf.Tensor of shape (batch_size, n_models)
        positive network outputs in ``[1, +inf]``

    Returns
    -------
    kl: tf.Tensor
        A single scalar representing :math:`D_{KL}(\mathrm{Dir}(\\alpha) | \mathrm{Dir}(1,1,\ldots,1) )`, shape (,)
    """

    # Extract number of models
    J = int(model_indices.shape[1])

    # Set-up ground-truth preserving prior
    alpha = alpha * (1 - model_indices) + model_indices
    beta = tf.ones((1, J), dtype=tf.float32)
    alpha0 = tf.reduce_sum(alpha, axis=1, keepdims=True)

    # Computation of KL
    kl = tf.reduce_sum((alpha - beta) * (tf.math.digamma(alpha) - tf.math.digamma(alpha0)), axis=1, keepdims=True) + \
        tf.math.lgamma(alpha0) - tf.reduce_sum(tf.math.lgamma(alpha), axis=1, keepdims=True) + \
        tf.reduce_sum(tf.math.lgamma(beta), axis=1, keepdims=True) - tf.math.lgamma(
        tf.reduce_sum(beta, axis=1, keepdims=True))
    loss = tf.reduce_mean(kl)
    return loss


def maximum_mean_discrepancy(source_samples, target_samples, weight=1., minimum=0., **_kwargs):
    """ This Maximum Mean Discrepancy (MMD) loss is calculated with a number of different Gaussian kernels.

    Parameters
    ----------
    x : tf.Tensor of shape (N, num_features)
    y:  tf.Tensor of shape  (M, num_features)
    weight: float, default: 1.0
        the weight of the MMD loss.
    minimum: float, default: 0.0
        lower loss bound

    Returns
    -------
    loss_value : tf.Tensor
        A scalar Maximum Mean Discrepancy, shape (,)
    """

    sigmas = [
        1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,
        1e3, 1e4, 1e5, 1e6
    ]
    gaussian_kernel = partial(_gaussian_kernel_matrix, sigmas=sigmas)
    loss_value = _mmd_kernel(source_samples, target_samples, kernel=gaussian_kernel)
    loss_value = tf.maximum(minimum, loss_value) * weight
    return loss_value


def _gaussian_kernel_matrix(x, y, sigmas):
    """ Computes a Gaussian Radial Basis Kernel between the samples of x and y.

    We create a sum of multiple gaussian kernels each having a width :math:`\sigma_i`.

    Parameters
    ----------
    x :  tf.Tensor of shape (M, num_features)
    y :  tf.Tensor of shape (N, num_features)
    sigmas : list(float)
        List which denotes the widths of each of the gaussians in the kernel.

    Returns
    -------
    kernel: tf.Tensor
        RBF kernel of shape [num_samples{x}, num_samples{y}]
    """
    def norm(v):
        return tf.reduce_sum(tf.square(v), 1)
    beta = 1. / (2. * (tf.expand_dims(sigmas, 1)))
    dist = tf.transpose(norm(tf.expand_dims(x, 2) - tf.transpose(y)))
    s = tf.matmul(beta, tf.reshape(dist, (1, -1)))
    kernel = tf.reshape(tf.reduce_sum(tf.exp(-s), 0), tf.shape(dist))
    return kernel


def _mmd_kernel(x, y, kernel=_gaussian_kernel_matrix):
    """ Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.

    Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of the distributions of x and y.

    Parameters
    ----------
    x      : tf.Tensor of shape (num_samples, num_features)
    y      : tf.Tensor of shape (num_samples, num_features)
    kernel : callable, default: _gaussian_kernel_matrix
        A function which computes the kernel in MMD.

    Returns
    -------
    loss : tf.Tensor
        squared maximum mean discrepancy loss, shape (,)
    """

    loss = tf.reduce_mean(kernel(x, x))  # lint error: sigmas unfilled
    loss += tf.reduce_mean(kernel(y, y))  # lint error: sigmas unfilled
    loss -= 2 * tf.reduce_mean(kernel(x, y))  # lint error: sigmas unfilled
    return loss
